---
title: "Prepare data (legacy datawarehouse)"
author: "Your Name"
date: "YYYY-MM-DD"
output: output_format
---

```{r setup, include=FALSE}

library(data.table)
library(nfportalutils)
library(googleAnalyticsR)
library(usagereports) 

syn_login()
```

## Query portal studies 

This queries data from the warehouse snapshot database -- it can take at least several hours for a funding agency such as NTAP. 

To connect to your warehouse snapshot, create a `config.yml` with the snapshot parameters from the [build](http://build-system-dw.sagebase.org:8080/login?from=%2Fjob%2FLaunch%2520warehouse%2520snapshot%2F).
If an older `config.yml` does not exist to reuse with updated parameters, use the helper `dw_config` to write out one for you to fill in.
The snapshot name and credentials should match what was specified during the build.
Also make sure that you are on the Sage VPN in order to connect to your snapshot.

### Connect
```{r}
con <- usagereports::connect_to_dw(config_file = "config.yml")
```

### Query data

Start and end dates of the query should be specified. 
It is suggested to use **quarterly intervals**, which seems to be faster than pulling data in one six-month batch. It also works better for any unexpected disconnections. 
Thus, there are two blocks for querying data.
```{r}

start_date <- "2023-03-01"
end_date <- "2023-06-01"
dir_name <- "2023-Q2" 
dir.create(dir_name)
setwd(dir_name)

query_data_by_funding_agency(con = con, 
                             start_date = start_date, 
                             end_date = end_date, 
                             disconnect = FALSE)
```


```{r}

start_date <- "2023-06-01"
end_date <- "2023-09-01"
dir_name <- "2023-Q3" 
dir.create(dir_name)
setwd(dir_name)

query_data_by_funding_agency(con = con, 
                             start_date = start_date, 
                             end_date = end_date, 
                             disconnect = FALSE)
```

### Transformations: Consolidate, deidentify, classify, and archive data

The raw query data can now be consolidated. 
The consolidated data is the intermediate data result to archive and pass to downstream integration/plotting functions.

During this transformation, user ids are masked and mappings added to the encrypted file `codes.csv`. 
This requires the [encryption key](https://www.synapse.org/#!Synapse:syn39770191/files/) on hand (obtainable through DCC staff-only project).
The updated `codes.csv` can be re-committed to the git repo. Here the path expects that it's several levels up.

```{r}

batch1 <- to_deidentified_export(data_dir = "2023-Q2",
                                 code_file = "../../../codes.csv",
                                 key_rds = "../../../usage_report_key.rds")

batch2 <- to_deidentified_export(data_dir = "2023-Q3",
                                 code_file = "../../../codes.csv",
                                 key_rds = "../../../usage_report_key.rds")

archive <- rbind(batch1, batch2)
```

We need to add a classification new column called `userGroup` and do a quick review before storing the data.
```{r}

archive[, userGroup := ifelse(userId < 0, "Sage", "regular")]
archive[, date := as.Date(date)]

# Sanity-check dates
archive[, .(first = min(date), last = max(date))]

# Review rows and columns
head(archive)
```

If all looks good, store the data.
```{r}
store_snapshot_data(name = "NTAP_issue-2_2023-Q2_2023-Q3", data = archive)
```

