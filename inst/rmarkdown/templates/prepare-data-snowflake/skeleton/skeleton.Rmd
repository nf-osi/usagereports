---
title: "Data prep for Snowflake data"
author: "Your Name"
date: "The Date"
output:
  html_document:
    df_print: paged
params:
  eval: yes
  eval_conditional: no
  data_dir: '.'
  start_date: "2022-09-01" # customize this date
  end_date: "2023-10-31"  # customize this date
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(DBI)
library(odbc)
library(usagereports)
library(data.table)

data_dir <- params$data_dir
```

## Query for the main data

### Generate main query

Use this helper to generate query scoped to projects with released data and whatever time frame applicable to the report.
```{r}

# Update code here to set your list of project ids of interest
ids <- c("syn123", "syn456") 

query <- query_filedownload_scoped(params$start_date, 
                                   params$end_date, 
                                   ids)

# Review the query
print(query)

```

(OPTIONAL) Save the query to version-controlled workspace.
```{r}

cat(query, file = "sf_filedownload_query.sql")

```

### Run query to get data in Rstudio

If drivers have been installed and confirmed to be set up properly, connect to Snowflake (go through the OAuth flow) and run the query. 
Otherwise, paste query from above into e.g. Snowflake Worksheets UI, export the result data, and skip to the section marked ALTERNATIVE. 
```{r}

domain <- "xxx"
uid <- "you@sagebase.org"

con <- DBI::dbConnect(
    drv = odbc::odbc(),
    dsn = "snowflake",
    server = glue::glue("{domain}.snowflakecomputing.com"),
    authenticator = "externalbrowser",
    uid = uid
)


```

Run these "USE" statements once to set role/db/table needed before running actual query...
```{r}

DBI::dbExecute(con, "USE ROLE DATA_ANALYTICS;")
DBI::dbExecute(con, "USE WAREHOUSE COMPUTE_XSMALL;")
DBI::dbExecute(con, "USE DATABASE SYNAPSE_DATA_WAREHOUSE;")
DBI::dbExecute(con, "USE SCHEMA SYNAPSE;")
```

Now do query.
```{r}

dat <- DBI::dbGetQuery(con, query)
dat <- as.data.table(dat)
# preview data
head(dat)

```


### (ALTERNATIVE WORKFLOW STEP) Run query somewhere else and load data back into Rstudio 

If you were able to do the default above, ignore/delete this section, otherwise change to `eval=TRUE`, customize the path variable below, then run block to read data into session.
```{r, eval=FALSE}

exported_data_path <- "path/to/data.csv"
dat <- fread(exported_data_path)
```

## Derive and preview stats and figures 

### Basic

#### Total download requests

```{r}

n_downloads <- nrow(dat)
n_downloads 

```

#### Total unique files

```{r}

n_files <- length(unique(dat$FILE_HANDLE_ID))
n_files 
```


#### Total unique users

```{r}

n_users <- length(unique(dat$USER_ID))
n_users 
```


#### Distribution of downloads by users

This answers questions like: 
- how many unique requests per user
- How different do users look/are there "power users"?

```{r}

user_download_profiles <- dat[, .N, by = USER_ID]
user_download_profiles

```

Find the mean/median download for a user. If a power distribution is shown above, these should be very different.
```{r}

median_download_per_user <- median(user_download_profiles$N)
mean_download_per_user <- round(mean(user_download_profiles$N))
  
cat("median:", median_download_per_user, "\n")
cat("mean:", mean_download_per_user, "\n")



```


#### Distribution of downloads by projects

Similarly to "power users", downloads can be variably distributed by project.

```{r}

project_download_profiles <- dat[, .N, by = PROJECT_ID]
project_download_profiles 

```

To get something like "20% of released projects constitute 90% of downloaded data files", arrange projects by descending downloads order, translate to % of total, calculate the cumulative percentage, and decide on desired cutoff.
```{r}

project_download_profiles <- project_download_profiles[order(N, decreasing = TRUE), ]

project_download_profiles[, Percent := N/n_downloads]
project_download_profiles[, Cumulative := cumsum(Percent)]
project_download_profiles
```


```{r}

top_3_project_prct_share <- round(project_download_profiles[3, Cumulative] * 100)
cat("The top 3 projects constitute", top_3_project_prct_share, "percent of downloaded data files", "\n")

top_5_project_prct_share <- round(project_download_profiles[5, Cumulative] * 100)
cat("The top 5 projects constitute", top_5_project_prct_share, "percent of downloaded data files", "\n")
  
```

```{r}

median_download_per_project <- median(project_download_profiles$N)
mean_download_per_project <- round(mean(project_download_profiles$N))

cat("median:", median_download_per_project, "\n")
cat("mean:", mean_download_per_project, "\n")

```

### More complex

#### Network of data users

```{r}

users_to_projects <- FD[, .N, by = .(USER_ID, PROJECT_ID)]
setnames(users_to_projects, c("userId", "project", "N"))

par(mar = c(1, 0, 1, 1))
plot_bipartite(users_to_projects,
               project_node_size = 18,
               user_node_size = 7)
project_node_color <- "#6fbeb8"
user_node_color <- "#af316c"

par(mar = c(0.5, 4, 0.5, 4))
legend('topright',
       legend = c("project", "user"),
       pt.cex = c(3,2),
       col = "black",
       pch=21,
       pt.bg= c(project_node_color, user_node_color),
       x.intersp = 1,
       y.intersp = 2,
       inset= -0.02,
       xpd = T)


```

### Comparisons with other data (e.g. past report periods)

#### X

```{r}


```

### Selectively write summary data

Write to file the summary results and tables of interest.
```{r}

results <- list(n_downloads = n_downloads, # unfiltered
                n_files = n_files, # unfiltered
                
                fn_downloads = fn_downloads, 
                fn_files = fn_files, 
                fn_users = fn_users,
                mean_download_per_user =  mean_download_per_user,
                mean_download_per_project = mean_download_per_project,
                top_3_project_prct_share  = top_3_project_prct_share,
                percent_projects_with_download = percent_projects_with_download)

yaml::write_yaml(results, "download_stats.yaml")

fwrite(users_to_projects, "user_network.csv")

```
